import React from 'react';
import { calculateFrontBiasedSimilarity, meetsMainThreshold, meetsVariationThreshold } from '../utils/similarity';

export type Cue = { role: string; text: string; audioUrl?: string; skipRecording?: boolean };
export type Script = Cue[];
export type Phase = 'idle' | 'entry' | 'ai-playing' | 'waiting' | 'user-recording' | 'waiting-for-confirmation' | 'done';
export type SubtitleKind = 'ai' | 'user-partial' | 'user-final' | null;

export interface TurnEngineConfig {
  script: Script;
  userRole: string;
  adlibMode: boolean;
  getIsPlaying: () => boolean;
  onPhase: (phase: Phase) => void;
  onSubtitle: (text: string, kind: SubtitleKind) => void;
  onError: (type: string, detail?: unknown) => void;
}

export interface TurnEngine {
  load: () => void;
  start: () => void;
  pause: () => void;
  resume: () => void;
  destroy: () => void;
  getIndex: () => number;
  manualNext: () => void;
  confirmAndNext: () => void;
}

// Constants
const VAD_SILENCE_THRESHOLD = 700; // ms
const VAD_CALIBRATION_TIME = 500; // ms (ë‹¨ì¶•)
const VAD_ADAPTIVE_MULTIPLIER = 1.5; // ê°ì†Œ
const VAD_MIN_THRESHOLD = 0.01; // ìµœì†Œ ì„ê³„ê°’
const SIMILARITY_THRESHOLD = 0.78;
const VARIATION_THRESHOLD = 0.60;

  // Global state
let currentIndex = 0;
let currentPhase: Phase = 'idle';
let isDestroyed = false;
let isPaused = false; // ì´ˆê¸°ê°’ì€ false (ì¬ìƒ ìƒíƒœ)
let lastPlayState = false; // ë§ˆì§€ë§‰ ì¬ìƒ ìƒíƒœ ì¶”ì 
let audioContext: AudioContext | null = null;
let analyser: AnalyserNode | null = null;
let microphone: MediaStreamAudioSourceNode | null = null;
let audioStream: MediaStream | null = null;
let dataArray: Uint8Array | null = null;
let vadInterval: NodeJS.Timeout | null = null;
let vadSilenceStart: number | null = null;
let noiseFloor = 0;
let vadCalibrationSamples: number[] = [];
let isVADCalibrated = false;
let currentUserText = '';
let isRecording = false;
let mediaRecorder: MediaRecorder | null = null;
let recordingTimeout: NodeJS.Timeout | null = null;
let config: TurnEngineConfig | null = null;
let isStoppedByUser = false; // ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ì •ì§€í–ˆëŠ”ì§€ ì¶”ì 
let hasVoiceStarted = false; // ìŒì„±ì´ ì‹œì‘ë˜ì—ˆëŠ”ì§€ ì¶”ì 
let voiceEndTime: number | null = null; // ë§ˆì§€ë§‰ ìŒì„±ì´ ê°ì§€ëœ ì‹œê°„

// Recording buffer for Whisper
let recordedChunks: BlobPart[] = [];

// Whisper í˜¸ì¶œ í—¬í¼
async function transcribeWithWhisper(blob: Blob, lang: string = 'ko'): Promise<string> {
  const form = new FormData();
  form.append('audio', blob, 'speech.webm');
  form.append('lang', lang);
  const res = await fetch('/api/stt', { method: 'POST', body: form });
  if (!res.ok) {
    const text = await res.text();
    throw new Error(`STT API failed: ${res.status} ${text}`);
  }
  const data = await res.json();
  return data.text || '';
}

// Audio playback or simulation
async function playOrSimulate(cue: Cue): Promise<void> {
  return new Promise((resolve) => {
    if (cue.audioUrl) {
      // Real audio playback
      const audio = new Audio(cue.audioUrl);
      audio.onloadeddata = () => {
        audio.play();
        audio.onended = () => resolve();
      };
      audio.onerror = () => {
        console.warn('Audio load failed, falling back to simulation');
        // Fallback to simulation
        const duration = Math.max(2000, cue.text.length * 100);
        setTimeout(resolve, duration);
      };
    } else {
      // Time simulation based on text length
      const duration = Math.max(2000, cue.text.length * 100);
      setTimeout(resolve, duration);
    }
  });
}

// Start recording with MediaRecorder + Whisper
async function startRecording(): Promise<void> {
  try {
    console.log('ğŸ¤ Starting voice recording with MediaRecorder + Whisper...');
    
    // 1. ë…¹ìŒ ì‹œì‘ ì‹œ í”Œë˜ê·¸ ì˜¬ë¦¬ê¸°
    isRecording = true;

    // Ensure AudioContext exists before any VAD work
    try {
      if (!audioContext) {
        const AC = (window as any).AudioContext || (window as any).webkitAudioContext;
        audioContext = new AC();
        console.log('ğŸ”Š AudioContext created', { sampleRate: audioContext?.sampleRate, state: audioContext?.state });
      } else if (audioContext.state === 'suspended') {
        await audioContext.resume();
        console.log('ğŸ”Š AudioContext resumed (pre-recording)');
      }
    } catch (e) {
      console.warn('âš ï¸ Failed to create/resume AudioContext', e);
    }

    // Request microphone access with detailed logging
    console.log('ğŸ” Requesting microphone access...');
    audioStream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: 44100
      } 
    });
    console.log('âœ… Microphone access granted, setting up recording...');

    // MediaRecorder ì„¤ì • ë° ì‹œì‘
    const mime = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') ? 'audio/webm;codecs=opus' : 'audio/webm';
    mediaRecorder = new MediaRecorder(audioStream, { mimeType: mime });
    recordedChunks = [];
    
    mediaRecorder.ondataavailable = (e) => {
      if (e.data && e.data.size > 0) recordedChunks.push(e.data);
    };
    
    mediaRecorder.onstart = () => {
      console.log('âœ… MediaRecorder started', { mimeType: mime });
    };
    
    mediaRecorder.onerror = (e: any) => {
      console.warn('âš ï¸ MediaRecorder error', e);
    };
    
    mediaRecorder.onstop = async () => {
      console.log('â¹ï¸ MediaRecorder stopped, sending to Whisper...', { chunks: recordedChunks.length, isStoppedByUser });
      const blob = new Blob(recordedChunks, { type: mime });
      try {
        const text = await transcribeWithWhisper(blob, 'ko');
        currentUserText = text || '';
        console.log('âœ… Whisper transcription result:', currentUserText);
        
        if (currentUserText) {
          config?.onSubtitle(currentUserText, 'user-final');
        }
        
        // ë…¹ìŒ ì •ë¦¬
        cleanupRecording();
        isRecording = false;
        
        // ëª¨ë‘ waiting-for-confirmation ìƒíƒœë¡œ ì„¤ì • (ê²°ê³¼ í‘œì‹œ)
        currentPhase = 'waiting-for-confirmation';
        config?.onPhase('waiting-for-confirmation');
        console.log('âœ… Recording completed, showing result to user');
        
        // ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ì •ì§€í•œ ê²½ìš°ë§Œ ëŒ€ê¸°, ìë™ ì¢…ë£ŒëŠ” 4ì´ˆ í›„ ì§„í–‰
        if (!isStoppedByUser) {
          // ìë™ ì¢…ë£Œëœ ê²½ìš° â†’ ê²°ê³¼ë¥¼ 4ì´ˆê°„ í‘œì‹œ í›„ ìë™ìœ¼ë¡œ ë‹¤ìŒ í„´ìœ¼ë¡œ
          console.log('â–¶ï¸ Auto-stopped, showing result for 4 seconds then proceeding...');
          setTimeout(() => {
            if (!isDestroyed && currentPhase === 'waiting-for-confirmation') {
              console.log('âœ… Auto-advancing to next cue');
              // waiting ë‹¨ê³„ ìƒëµí•˜ê³  ë°”ë¡œ ë‹¤ìŒ cueë¡œ
              nextCue();
            }
          }, 4000);
        } else {
          console.log('â¸ï¸ Waiting for user confirmation (manual stop)');
        }
      } catch (err) {
        console.error('âŒ Whisper transcription failed', err);
        config?.onError('stt-failed', err);
        // ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ê²°ê³¼ë¥¼ í‘œì‹œí•˜ê³  waiting-for-confirmation ìƒíƒœ ìœ ì§€
        cleanupRecording();
        isRecording = false;
        currentPhase = 'waiting-for-confirmation';
        config?.onPhase('waiting-for-confirmation');
        // ì—ëŸ¬ ë©”ì‹œì§€ë„ í‘œì‹œí•˜ê³  4ì´ˆ í›„ ìë™ ì§„í–‰
        setTimeout(() => {
          if (!isDestroyed && currentPhase === 'waiting-for-confirmation') {
            console.log('âœ… Auto-advancing to next cue after error');
            currentPhase = 'waiting';
            config?.onPhase('waiting');
            nextCue();
          }
        }, 4000);
      }
    };
    
    mediaRecorder.start();
    console.log('âœ… Recording started successfully (MediaRecorder)');

    // ìŒì„± ê°ì§€ ì‹œì‘ ë° ì¹¨ë¬µ íƒ€ì´ë¨¸ ì´ˆê¸°í™”
    hasVoiceStarted = false;
    voiceEndTime = null;

    // VADëŠ” ìŒì„±ì„ ê°ì§€í•œ í›„ 3ì´ˆ ì¹¨ë¬µ ì‹œ ìë™ ì¢…ë£Œ
    await setupVAD(true); // ìë™ ì¢…ë£Œ í™œì„±í™”

  } catch (error) {
    console.error('âŒ Recording setup failed:', error);
    if (error instanceof Error) {
      if (error.name === 'NotAllowedError') {
        config?.onError('mic-permission-denied');
      } else if (error.name === 'NotFoundError') {
        config?.onError('mic-permission-denied');
      } else {
        config?.onError('recording-setup-failed', error.message);
      }
    }
    throw error;
  }
}

// Enhanced VAD setup with time domain and better debugging
async function setupVAD(enableAutoStop: boolean = true): Promise<void> {
  try {
    console.log('ğŸ” Setting up Voice Activity Detection (Time Domain)...', { enableAutoStop });

    if (!audioContext) {
      throw new Error('AudioContext is not initialized. Create it in startRecording() before setupVAD().');
    }
    
    // AudioContext ìƒíƒœ í™•ì¸ ë° ì¬ê°œ
    if (audioContext?.state === 'suspended') {
      await audioContext.resume();
      console.log('ğŸ”Š AudioContext resumed');
    }
    
    analyser = audioContext.createAnalyser();
    microphone = audioContext.createMediaStreamSource(audioStream!);

    // VAD ì„¤ì • ìµœì í™”
    analyser.fftSize = 1024; // 2048ì—ì„œ 1024ë¡œ ê°ì†Œ (ë” ë¹ ë¥¸ ì²˜ë¦¬)
    analyser.smoothingTimeConstant = 0.1; // 0.3ì—ì„œ 0.1ë¡œ ê°ì†Œ (ë” ë¯¼ê°)
    analyser.minDecibels = -90;
    analyser.maxDecibels = -10;
    
    // ì˜¤ë””ì˜¤ íŒŒì´í”„ ì—°ê²° í™•ì¸
    microphone.connect(analyser);
    console.log('ğŸ”— Audio pipeline connected:', {
      sourceNode: !!microphone,
      analyserNode: !!analyser,
      audioContextState: audioContext?.state,
      sampleRate: audioContext?.sampleRate
    });

    dataArray = new Uint8Array(analyser.frequencyBinCount);

    vadCalibrationSamples = [];
    isVADCalibrated = false;
    const calibrationStartTime = Date.now();

    console.log('ğŸ¯ VAD calibration started (500ms)...');

    vadInterval = setInterval(() => {
      if (!analyser || !dataArray || isDestroyed) {
        return;
      }

      if (currentPhase !== 'user-recording' || !isRecording) {
        // phaseê°€ ë³€ê²½ë˜ë©´ intervalì€ ê³„ì† ì‹¤í–‰ë˜ì§€ë§Œ early return
        return;
      }

      // ì‹œê°„ ë„ë©”ì¸ ë°ì´í„° ì‚¬ìš© (ë” ì •í™•í•œ ìŒì„± ê°ì§€)
      analyser.getByteTimeDomainData(dataArray as any);

      // RMS ê³„ì‚° (ì‹œê°„ ë„ë©”ì¸)
      let sum = 0;
      let peak = 0;
      for (let i = 0; i < dataArray.length; i++) {
        const normalized = (dataArray[i] - 128) / 128;
        sum += normalized * normalized;
        peak = Math.max(peak, Math.abs(normalized));
      }
      const rms = Math.sqrt(sum / dataArray.length);

      const currentTime = Date.now();
      const isCalibrating = currentTime - calibrationStartTime < VAD_CALIBRATION_TIME;

      if (isCalibrating) {
        vadCalibrationSamples.push(rms);
        console.log('ğŸ¯ VAD calibrating...', { 
          rms: rms.toFixed(4), 
          peak: peak.toFixed(4),
          samples: vadCalibrationSamples.length,
          timeRemaining: VAD_CALIBRATION_TIME - (currentTime - calibrationStartTime),
          rawData: Array.from(dataArray.slice(0, 10)) // ì²˜ìŒ 10ê°œ ìƒ˜í”Œ í™•ì¸
        });
      } else if (!isVADCalibrated) {
        if (vadCalibrationSamples.length > 0) {
          noiseFloor = vadCalibrationSamples.reduce((a, b) => a + b, 0) / vadCalibrationSamples.length;
          // ìµœì†Œ ì„ê³„ê°’ ë³´ì¥
          noiseFloor = Math.max(noiseFloor, VAD_MIN_THRESHOLD);
          isVADCalibrated = true;
          console.log('âœ… VAD calibrated:', { 
            noiseFloor: noiseFloor.toFixed(4), 
            threshold: (noiseFloor * VAD_ADAPTIVE_MULTIPLIER).toFixed(4),
            minThreshold: VAD_MIN_THRESHOLD,
            samples: vadCalibrationSamples.length,
            calibrationData: vadCalibrationSamples.slice(0, 5) // ì²˜ìŒ 5ê°œ ìƒ˜í”Œ í™•ì¸
          });
        } else {
          console.warn('âš ï¸ No calibration samples collected, using default threshold');
          noiseFloor = VAD_MIN_THRESHOLD;
          isVADCalibrated = true;
        }
      } else {
        const threshold = Math.max(noiseFloor * VAD_ADAPTIVE_MULTIPLIER, VAD_MIN_THRESHOLD);
        const isVoiceDetected = rms > threshold;

        // ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸ (ì²˜ìŒ ëª‡ ë²ˆë§Œ)
        if (Math.random() < 0.1) { // 10% í™•ë¥ ë¡œë§Œ ë¡œê·¸
          console.log('ğŸ¯ VAD check:', { 
            rms: rms.toFixed(4), 
            peak: peak.toFixed(4),
            threshold: threshold.toFixed(4), 
            isVoiceDetected, 
            noiseFloor: noiseFloor.toFixed(4),
            silenceDuration: vadSilenceStart ? Date.now() - vadSilenceStart : 0,
            rawSample: dataArray[0] // ì²« ë²ˆì§¸ ìƒ˜í”Œ ê°’
          });
        }

        // enableAutoStopì´ falseë©´ ìë™ ì¢…ë£Œí•˜ì§€ ì•ŠìŒ
        if (!enableAutoStop) {
          return; // ì‹¤ì‹œê°„ ë ˆë²¨ í‘œì‹œë§Œì„ ìœ„í•´ VAD ê³„ì† ì‹¤í–‰
        }
        
        if (isVoiceDetected) {
          // ìŒì„± ê°ì§€ë¨
          hasVoiceStarted = true; // ìŒì„±ì´ ì‹œì‘ë˜ì—ˆë‹¤ê³  í‘œì‹œ
          voiceEndTime = null; // ì¹¨ë¬µ ì‹œê°„ ë¦¬ì…‹
          vadSilenceStart = null;
        } else {
          // ì¹¨ë¬µ ê°ì§€ë¨
          if (hasVoiceStarted) {
            // ì´ë¯¸ ìŒì„±ì´ ì‹œì‘ëœ ì ì´ ìˆìœ¼ë©´, ì¹¨ë¬µ ì‹œê°„ ì¹´ìš´íŠ¸ ì‹œì‘
            if (voiceEndTime === null) {
              voiceEndTime = Date.now();
              console.log('ğŸ”‡ Silence started after voice, starting 2 second timer...');
            } else if (Date.now() - voiceEndTime >= 2000) {
              // 2ì´ˆ ì¹¨ë¬µ ì§€ì† â†’ ìë™ ì¢…ë£Œ
              console.log('ğŸ”‡ 2 seconds of silence after voice detected â€” stopping recorder for STT');
              if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
              } else {
                checkRecordingCompletion();
              }
            }
          }
        }
      }
    }, 50); // 100msì—ì„œ 50msë¡œ ê°ì†Œ (ë” ë¹ ë¥¸ ë°˜ì‘)

    console.log('âœ… VAD setup completed');

  } catch (error) {
    console.warn('âš ï¸ VAD setup failed:', error);
  }
}

// Cleanup recording resources
function cleanupRecording(): void {
  console.log('ğŸ§¹ Cleaning up recording...');
  
  if (recordingTimeout) {
    clearTimeout(recordingTimeout);
    recordingTimeout = null;
  }

  if (vadInterval) {
    clearInterval(vadInterval);
    vadInterval = null;
  }

  // MediaRecorder ì •ë¦¬
  if (mediaRecorder && mediaRecorder.state !== 'inactive') {
    try { 
      mediaRecorder.stop(); 
    } catch (e) {
      console.warn('âš ï¸ Error stopping MediaRecorder:', e);
    }
    mediaRecorder = null;
  }
  recordedChunks = [];

  if (audioStream) {
    audioStream.getTracks().forEach(track => track.stop());
    audioStream = null;
  }

  if (audioContext) {
    audioContext.close();
    audioContext = null;
  }

  analyser = null;
  microphone = null;
  dataArray = null;
  isRecording = false;
  vadSilenceStart = null;
  noiseFloor = 0;
  vadCalibrationSamples = [];
  isVADCalibrated = false;
  
  console.log('âœ… Recording cleanup completed');
}

// Cleanup audio resources
function cleanupAudio(): void {
  console.log('ğŸ§¹ Cleaning up audio...');
  if (audioContext) {
    audioContext.close();
    audioContext = null;
  }
  console.log('âœ… Audio cleanup completed');
}

// Check if recording should be completed
function checkRecordingCompletion(): void {
  console.log('ğŸ” CheckRecordingCompletion called:', {
    isRecording,
    currentUserText: currentUserText || '(empty)',
    currentPhase
  });

  if (!isRecording) {
    console.log('â­ï¸ CheckRecordingCompletion skipped: not recording');
    return;
  }

  if (!currentUserText || !currentUserText.trim()) {
    console.log('âš ï¸ No STT text returned (empty). Staying in waiting-for-confirmation to show result.');
    return; // completeRecordingì„ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - ì´ë¯¸ mediaRecorder.onstopì—ì„œ ì²˜ë¦¬ë¨
  }

  console.log('âœ… checkRecordingCompletion: User speech received from Whisper');
  console.log('ğŸ¯ User said:', currentUserText.trim());
  // completeRecordingì„ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - mediaRecorder.onstopì—ì„œ ì²˜ë¦¬
}

// Complete recording
function completeRecording(): void {
  console.log('âœ… Completing recording with text:', currentUserText || '(empty)');
  
  // ì‹¤ì œ ë…¹ìŒëœ í…ìŠ¤íŠ¸ë¥¼ ê³„ì† ë³´ì—¬ì¤Œ (í•˜ë“œì½”ë”©ëœ 'ì‚¬ìš©ì ëŒ€ì‚¬ ì™„ë£Œ' í…ìŠ¤íŠ¸ ì œê±°)
  // í˜„ì¬ ìë§‰ì€ ì´ë¯¸ onSubtitleë¡œ ì„¤ì •ë˜ì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€

  // VAD ì¸í„°ë²Œì„ ë¨¼ì € ì •ë¦¬ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
  if (vadInterval) {
    clearInterval(vadInterval);
    vadInterval = null;
    console.log('ğŸ›‘ VAD interval cleared');
  }

  cleanupRecording();
  
  // isPaused ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì¬ìƒ ì¤‘ì´ë©´ ë‹¤ìŒ cueë¡œ ì§„í–‰
  console.log('ğŸ¯ Complete recording - checking state:', { isPaused, getIsPlaying: config?.getIsPlaying?.() });
  
  if (!isPaused && config?.getIsPlaying()) {
    console.log('âœ… Recording completed, advancing to next cue');
    setTimeout(() => {
      setPhase('waiting'); // waitingìœ¼ë¡œ ì „í™˜í•˜ë©´ setPhaseì—ì„œ nextCue í˜¸ì¶œ
    }, 100);
  } else {
    console.log('â¸ï¸ Recording completed but paused, staying in waiting');
    setPhase('waiting');
  }
}

// Trigger ad-lib mode
async function triggerAdlib(): Promise<void> {
  console.log('ğŸ­ Triggering ad-lib mode...');
  
  try {
    const response = await fetch('/api/next-lines', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        history: config?.script.slice(0, currentIndex + 1),
        userText: currentUserText
      })
    });

    if (response.ok) {
      const data = await response.json();
      console.log('ğŸ­ Ad-lib response:', data);
      // Add new lines to script
      if (data.lines && Array.isArray(data.lines)) {
        config?.script.push(...data.lines);
      }
    }
  } catch (error) {
    console.warn('âš ï¸ Ad-lib request failed:', error);
  }

  completeRecording();
}

// Move to next cue
function nextCue(): void {
  console.log('â­ï¸ nextCue() called, currentIndex:', currentIndex, 'currentPhase:', currentPhase);
  
  if (currentIndex >= (config?.script.length || 0) - 1) {
    console.log('ğŸ Script completed, setting phase to done');
    setPhase('done');
    return;
  }

  currentIndex++;
  console.log('â­ï¸ Processing next cue, new index:', currentIndex);
  processCurrentCue();
}

// Process current cue
function processCurrentCue(): void {
  const cue = config?.script[currentIndex];
  if (!cue) {
    console.log('âŒ No cue found at index:', currentIndex);
    return;
  }

  console.log('ğŸ¬ Processing cue', currentIndex, ':', {
    role: cue.role,
    text: cue.text,
    userRole: config?.userRole
  });
  
  console.log(`ğŸ” Role comparison: cue.role="${cue.role}", userRole="${config?.userRole}", match=${cue.role === config?.userRole}`);

  if (cue.role === config?.userRole) {
    // íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ëŒ€ì‚¬ëŠ” ë…¹ìŒ ìŠ¤í‚µ
    if (cue.skipRecording) {
      console.log('ğŸ‘¤ User turn (special character only) - skipping recording');
      setPhase('user-recording'); // ë‚´ ì°¨ë¡€ ë””ìì¸ ìœ ì§€
      config?.onSubtitle(cue.text, 'ai');
      
      // 2ì´ˆ ëŒ€ê¸° í›„ ë‹¤ìŒ í„´ìœ¼ë¡œ (waiting ë‹¨ê³„ ì—†ì´ ë°”ë¡œ)
      setTimeout(() => {
        console.log('âœ… Special character turn completed, moving to next...');
        if (!isDestroyed && !isPaused) {
          nextCue();
        }
      }, 2000);
    } else {
      console.log('ğŸ‘¤ User turn starting...');
      setPhase('user-recording');
      config?.onSubtitle(cue.text, 'ai');
      // ìë™ìœ¼ë¡œ ë…¹ìŒ ì‹œì‘
      startRecording().catch(error => {
        console.error('âŒ Recording start failed:', error);
        setPhase('waiting');
      });
    }
  } else {
    console.log('ğŸ¤– AI turn starting...');
    setPhase('ai-playing');
    config?.onSubtitle(cue.text, 'ai');
    
    playOrSimulate(cue).then(() => {
      console.log('ğŸ¤– AI turn completed, moving to next...');
      // ì¼ì‹œì •ì§€ ìƒíƒœê°€ ì•„ë‹ˆë©´ ë‹¤ìŒìœ¼ë¡œ ì§„í–‰
      if (!isDestroyed && !isPaused) {
        setTimeout(() => {
          if (!isDestroyed && !isPaused) {
            nextCue();
          }
        }, 200);
      } else {
        console.log('â¸ï¸ AI turn ended but paused, not moving to next');
      }
    });
  }
}

// Check play state and handle accordingly
function checkPlayState(): void {
  if (!config) return;
  
  const shouldBePlaying = config.getIsPlaying();
  console.log('ğŸ® Play state check:', { shouldBePlaying, currentPhase });
  
  if (shouldBePlaying && currentPhase === 'idle') {
    console.log('â–¶ï¸ Starting from idle state');
    setPhase('entry');
  } else if (!shouldBePlaying && currentPhase !== 'idle' && currentPhase !== 'done') {
    console.log('â¸ï¸ Pausing from active state');
    setPhase('waiting');
  }
}

// Set phase with logging
function setPhase(phase: Phase): void {
  console.log('ğŸ”„ Phase change:', currentPhase, '->', phase);
  currentPhase = phase;
  config?.onPhase(phase);

  // When entering 'waiting', schedule next cue automatically if playing
  if (phase === 'waiting') {
    setTimeout(() => {
      if (isDestroyed) return;
      console.log('â¸ï¸ Waiting state entered, checking conditions:', { 
        isPaused, 
        getIsPlaying: config?.getIsPlaying?.(), 
        shouldAdvance: !isPaused && config?.getIsPlaying?.() 
      });
      
      if (!config?.getIsPlaying() || isPaused) {
        console.log('â¸ï¸ Still paused; not advancing from waiting.');
        return;
      }
      console.log('â­ï¸ Advancing from waiting to next cue...');
      nextCue();
    }, 200); // small cushion between phases
  }
}

// Create Turn Engine
export function createTurnEngine(engineConfig: TurnEngineConfig): TurnEngine {
  config = engineConfig;
  currentIndex = 0; // ìƒˆë¡œ ì‹œì‘í•  ë•ŒëŠ” 0ë¶€í„°
  currentPhase = 'idle';
  isDestroyed = false;
  isPaused = false; // ì—¬ê¸°ì„œ ì´ˆê¸°í™”

  console.log('ğŸš€ Turn Engine created with config:', {
    scriptLength: config.script.length,
    userRole: config.userRole,
    adlibMode: config.adlibMode,
    currentIndex: currentIndex // í˜„ì¬ ìœ„ì¹˜ ë¡œê¹…
  });

  return {
    load: () => {
      console.log('ğŸ“¥ Turn Engine load called');
      if (currentPhase === 'idle') {
        setPhase('entry');
      }
    },

    start: () => {
      console.log('â–¶ï¸ Turn Engine start called, current phase:', currentPhase);
      
      isPaused = false;
      
      if (currentPhase === 'idle') {
        console.log('ğŸš€ Starting from idle, setting phase to entry');
        setPhase('entry');
        // entryì—ì„œ ë°”ë¡œ ì²« ë²ˆì§¸ cue ì²˜ë¦¬
        setTimeout(() => {
          console.log('ğŸ¬ Processing first cue after entry');
          processCurrentCue();
        }, 100);
      } else if (currentPhase === 'waiting') {
        console.log('ğŸ¬ Resuming from waiting state');
        processCurrentCue();
      } else {
        console.log('âš ï¸ Start called but phase is not idle or waiting:', currentPhase);
      }
    },

    pause: () => {
      console.log('â¸ï¸ Turn Engine pause called, current phase:', currentPhase);
      
      isPaused = true;
      
      if (currentPhase === 'user-recording') {
        cleanupRecording();
        setPhase('waiting');
      } else if (currentPhase === 'ai-playing') {
        // AI ì°¨ë¡€ê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì¼ì‹œì •ì§€
        console.log('â¸ï¸ Waiting for AI turn to complete before pausing...');
        // playOrSimulateì´ ëë‚˜ë©´ waiting ìƒíƒœë¡œ ì „í™˜ë˜ëŠ”ë°, isPaused í”Œë˜ê·¸ë¡œ ì¸í•´ ìë™ìœ¼ë¡œ ì •ì§€ë¨
      } else if (currentPhase === 'waiting') {
        // ì´ë¯¸ ëŒ€ê¸° ìƒíƒœì´ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ)
        console.log('â¸ï¸ Already in waiting state');
      } else if (currentPhase === 'entry') {
        // entry ìƒíƒœì—ì„œ ì¼ì‹œì •ì§€í•˜ë©´ idleë¡œ
        setPhase('idle');
      }
      // idleì´ë‚˜ done ìƒíƒœëŠ” ì´ë¯¸ ì¼ì‹œì •ì§€ ìƒíƒœ
    },

    resume: () => {
      console.log('â–¶ï¸ Turn Engine resume called');
      if (currentPhase === 'waiting') {
        processCurrentCue();
      }
    },

    destroy: () => {
      console.log('ğŸ’¥ Turn Engine destroy called');
      isDestroyed = true;
      cleanupRecording();
      cleanupAudio();
      currentPhase = 'idle';
    },

    getIndex: () => currentIndex,

    manualNext: () => {
      console.log('ğŸ‘† Manual next called');
      if (currentPhase === 'user-recording') {
        completeRecording();
      }
      nextCue();
    },
    
    confirmAndNext: () => {
      console.log('âœ… User confirmed, moving to next cue');
      if (currentPhase === 'waiting-for-confirmation') {
        isStoppedByUser = false;
        nextCue(); // waiting ë‹¨ê³„ ìƒëµí•˜ê³  ë°”ë¡œ ë‹¤ìŒìœ¼ë¡œ
      }
    }
  };
}

// React hook for Turn Engine
export function useTurnEngine(config: TurnEngineConfig) {
  const engineRef = React.useRef<TurnEngine | null>(null);
  const lastPlayStateRef = React.useRef<boolean>(false);

  React.useEffect(() => {
    engineRef.current = createTurnEngine(config);
    return () => {
      if (engineRef.current) {
        engineRef.current.destroy();
      }
    };
  }, []);

  // ì´ë²¤íŠ¸ ê¸°ë°˜ play state ëª¨ë‹ˆí„°ë§ (í´ë§ ì œê±°)
  React.useEffect(() => {
    if (!engineRef.current) return;

    const shouldBePlaying = config.getIsPlaying();
    const lastPlayState = lastPlayStateRef.current;

    // ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ë°˜ì‘
    if (shouldBePlaying !== lastPlayState) {
      console.log('ğŸ® Play state changed:', { from: lastPlayState, to: shouldBePlaying });

      if (shouldBePlaying) {
        engineRef.current.start();
      } else {
        engineRef.current.pause();
      }

      lastPlayStateRef.current = shouldBePlaying;
    }
  }, [config.getIsPlaying]); // config.getIsPlayingì´ ë³€ê²½ë  ë•Œë§Œ ì‹¤í–‰

  return engineRef.current;
}
