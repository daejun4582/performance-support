import React from 'react';
import { calculateFrontBiasedSimilarity, meetsMainThreshold, meetsVariationThreshold } from '../utils/similarity';
import { getMediaPaths } from '../utils/media-path';

export type Cue = { role: string; text: string; audioUrl?: string; videoUrl?: string; skipRecording?: boolean };
export type Script = Cue[];
export type Phase = 'idle' | 'entry' | 'ai-playing' | 'waiting' | 'user-recording' | 'waiting-for-confirmation' | 'done';
export type SubtitleKind = 'ai' | 'user-partial' | 'user-final' | null;

export interface TurnEngineConfig {
  script: Script;
  userRole: string;
  adlibMode: boolean;
  getIsPlaying: () => boolean;
  onPhase: (phase: Phase) => void;
  onSubtitle: (text: string, kind: SubtitleKind, cueIndex?: number) => void;
  onError: (type: string, detail?: unknown) => void;
  videoContainer?: HTMLElement | null; // ë¹„ë””ì˜¤ë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ
  getCurrentSettings?: () => { sliderValue: number; selectedPersonality: string; hasCustomImage: boolean }; // ë™ì  ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  workIndex?: number; // work1 or work2
  opponentGender?: 'male' | 'female'; // ìƒëŒ€ì—­ ì„±ë³„
  hasCustomImage?: boolean; // ì–¼êµ´ ì„¤ì • ì—¬ë¶€
}

export interface TurnEngine {
  load: () => void;
  start: () => void;
  pause: () => void;
  resume: () => void;
  destroy: () => void;
  getIndex: () => number;
  manualNext: () => void;
  confirmAndNext: () => void;
}

// Constants
const VAD_SILENCE_THRESHOLD = 700; // ms
const VAD_CALIBRATION_TIME = 500; // ms (ë‹¨ì¶•)
const VAD_ADAPTIVE_MULTIPLIER = 1.5; // ê°ì†Œ
const VAD_MIN_THRESHOLD = 0.01; // ìµœì†Œ ì„ê³„ê°’
const SIMILARITY_THRESHOLD = 0.78;
const VARIATION_THRESHOLD = 0.60;

  // Global state
let currentIndex = 0;
let currentPhase: Phase = 'idle';
let isDestroyed = false;
let isPaused = false; // ì´ˆê¸°ê°’ì€ false (ì¬ìƒ ìƒíƒœ)
let lastPlayState = false; // ë§ˆì§€ë§‰ ì¬ìƒ ìƒíƒœ ì¶”ì 
let pendingWhisperRequests: Set<Promise<void>> = new Set(); // ì§„í–‰ ì¤‘ì¸ Whisper ìš”ì²­ ì¶”ì 
let audioContext: AudioContext | null = null;
let analyser: AnalyserNode | null = null;
let microphone: MediaStreamAudioSourceNode | null = null;
let audioStream: MediaStream | null = null;
let dataArray: Uint8Array | null = null;
let vadInterval: NodeJS.Timeout | null = null;
let vadSilenceStart: number | null = null;
let noiseFloor = 0;
let vadCalibrationSamples: number[] = [];
let isVADCalibrated = false;
let currentUserText = '';
let isRecording = false;
let mediaRecorder: MediaRecorder | null = null;
let recordingTimeout: NodeJS.Timeout | null = null;
let config: TurnEngineConfig | null = null;
let isStoppedByUser = false; // ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ì •ì§€í–ˆëŠ”ì§€ ì¶”ì 
let hasVoiceStarted = false; // ìŒì„±ì´ ì‹œì‘ë˜ì—ˆëŠ”ì§€ ì¶”ì 
let voiceEndTime: number | null = null; // ë§ˆì§€ë§‰ ìŒì„±ì´ ê°ì§€ëœ ì‹œê°„

// Recording buffer for Whisper
let recordedChunks: BlobPart[] = [];

// Whisper í˜¸ì¶œ í—¬í¼
async function transcribeWithWhisper(blob: Blob, lang: string = 'ko'): Promise<string> {
  const form = new FormData();
  form.append('audio', blob, 'speech.webm');
  form.append('lang', lang);
  const res = await fetch('/api/stt', { method: 'POST', body: form });
  if (!res.ok) {
    const text = await res.text();
    throw new Error(`STT API failed: ${res.status} ${text}`);
  }
  const data = await res.json();
  return data.text || '';
}

// Current media instances for pause/resume
let currentAudio: HTMLAudioElement | null = null;
let currentVideo: HTMLVideoElement | null = null;
let currentMediaTimeout: NodeJS.Timeout | null = null;
let currentMediaResolve: (() => void) | null = null;
let pausedMediaTime = 0; // ì¼ì‹œì •ì§€ëœ ì‹œì ì˜ ì¬ìƒ ì‹œê°„
let simulationStartTime = 0; // ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ì‹œê°„
let simulationDuration = 0; // ì‹œë®¬ë ˆì´ì…˜ ì „ì²´ ì‹œê°„
let simulationElapsedTime = 0; // ì‹œë®¬ë ˆì´ì…˜ ê²½ê³¼ ì‹œê°„

// Media playback (audio/video) or simulation
async function playOrSimulate(cue: Cue, resumeFromTime: number = 0): Promise<void> {
  return new Promise((resolve) => {
    currentMediaResolve = resolve;
    
    console.log('ğŸ¬ [playOrSimulate] Starting with:', {
      hasVideoUrl: !!cue.videoUrl,
      videoUrl: cue.videoUrl,
      hasAudioUrl: !!cue.audioUrl,
      audioUrl: cue.audioUrl,
      resumeFromTime
    });
    
    // Videoê°€ ìˆìœ¼ë©´ ë¹„ë””ì˜¤ ìš°ì„ 
    if (cue.videoUrl) {
      console.log('ğŸ“¹ [playOrSimulate] Creating video element for:', cue.videoUrl);
      currentVideo = document.createElement('video');
      
      // ìºì‹œ ìš°íšŒë¥¼ ìœ„í•´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ (ì„¤ì • ë³€ê²½ ì‹œ ê°•ì œ ìƒˆë¡œê³ ì¹¨)
      const videoUrlWithCacheBust = cue.videoUrl + (cue.videoUrl.includes('?') ? '&' : '?') + '_t=' + Date.now();
      
      currentVideo.src = videoUrlWithCacheBust;
      currentVideo.style.width = '100%';
      currentVideo.style.height = '100%';
      currentVideo.style.objectFit = 'cover';
      
      // ì˜¤ë””ì˜¤ê°€ ìˆìœ¼ë©´ ë¹„ë””ì˜¤ ìŒì†Œê±° (ì†ë„ ì¡°ì ˆëœ ì˜¤ë””ì˜¤ ì‚¬ìš©)
      // ë¹ˆ ë¬¸ìì—´ ì²´í¬ ê°•í™”
      const hasAudio = cue.audioUrl && cue.audioUrl.trim().length > 0;
      if (hasAudio) {
        currentVideo.muted = true;
      }
      
      // videoContainerì— ì¶”ê°€
      if (config?.videoContainer) {
        // videoContainerê°€ DOMì— ìˆëŠ”ì§€ í™•ì¸
        if (!config.videoContainer.parentNode) {
          console.warn('âš ï¸ videoContainer is not in DOM, video may not be visible');
        }
        
        // ê¸°ì¡´ ë¹„ë””ì˜¤ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        const existingVideos = config.videoContainer.querySelectorAll('video');
        existingVideos.forEach(video => video.remove());
        
        // ìƒˆ ë¹„ë””ì˜¤ ì¶”ê°€
        config.videoContainer.appendChild(currentVideo);
        console.log('âœ… [playOrSimulate] Video element added to container');
        
        // ë¹„ë””ì˜¤ê°€ ì‹¤ì œë¡œ DOMì— ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if (!currentVideo.parentNode) {
          console.error('âŒ Video element was not added to container');
        }
      } else {
        console.warn('âš ï¸ No videoContainer provided, video will not be visible');
        // videoContainerê°€ ì—†ì–´ë„ ë¹„ë””ì˜¤ëŠ” ê³„ì† ë¡œë“œ (ë‚˜ì¤‘ì— ì¶”ê°€ë  ìˆ˜ ìˆìŒ)
      }
      
      // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì • í›„ ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œ ì‹œì‘
      let playbackStarted = false;
      
      // ì¬ìƒ ì‹œì‘ í•¨ìˆ˜ (ì¤‘ë³µ ë°©ì§€)
      const startPlayback = () => {
        if (playbackStarted || !currentVideo) return;
        playbackStarted = true;
        
        console.log('â–¶ï¸ [playOrSimulate] Starting video playback');
        
        // ì˜¤ë””ì˜¤ê°€ ìˆìœ¼ë©´ ì˜¤ë””ì˜¤ë„ í•¨ê»˜ ì¬ìƒ
        if (hasAudio) {
          console.log('ğŸ”Š [playOrSimulate] Has audio, playing both video and audio');
          // ì˜¤ë””ì˜¤ì™€ ë¹„ë””ì˜¤ ëª¨ë‘ ì‹œì‘
          currentVideo.play().catch(err => {
            console.error('âŒ Video play failed:', err);
          });
          playAudio(cue.audioUrl!, () => {
            // ì˜¤ë””ì˜¤ ì¢…ë£Œ ì‹œ ë¹„ë””ì˜¤ë„ ì •ë¦¬í•˜ê³  ì¢…ë£Œ
            if (currentVideo) {
              currentVideo.pause();
            }
            cleanup();
            resolve();
          }, resumeFromTime, true); // hasVideo: true ì „ë‹¬
          
          // ë¹„ë””ì˜¤ê°€ ë¨¼ì € ëë‚˜ë„ ì˜¤ë””ì˜¤ëŠ” ê³„ì† ì¬ìƒ (onended í•¸ë“¤ëŸ¬ ì„¤ì • ì•ˆ í•¨)
        } else {
          // ì˜¤ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ë¹„ë””ì˜¤ ì¢…ë£Œ ì‹œ resolve
          currentVideo.play().catch(err => {
            console.error('âŒ Video play failed:', err);
          });
          currentVideo.onended = () => {
            cleanup();
            resolve();
          };
        }
      };
      
      // resumeFromTimeì´ ìˆìœ¼ë©´ seeked ì´ë²¤íŠ¸ì—ì„œ ì¬ìƒ ì‹œì‘
      if (resumeFromTime > 0) {
        currentVideo.onseeked = () => {
          console.log('âœ… [playOrSimulate] Video seeked to:', resumeFromTime);
          startPlayback();
        };
      }
      
      currentVideo.onloadedmetadata = () => {
        if (currentVideo && resumeFromTime > 0) {
          // ì €ì¥ëœ ì‹œê°„ë¶€í„° ì¬ìƒ (ë©”íƒ€ë°ì´í„° ë¡œë“œ í›„ ì„¤ì •)
          console.log('â° [playOrSimulate] Setting currentTime to:', resumeFromTime);
          currentVideo.currentTime = resumeFromTime;
        } else if (resumeFromTime === 0) {
          // resumeFromTimeì´ 0ì´ë©´ ì¦‰ì‹œ ì¬ìƒ ì‹œì‘
          startPlayback();
        }
      };
      
      // canplay ì´ë²¤íŠ¸ì—ì„œë„ ì‹œê°„ ì„¤ì • (ë” ì•ˆì „í•¨)
      currentVideo.oncanplay = () => {
        if (currentVideo && resumeFromTime > 0 && Math.abs(currentVideo.currentTime - resumeFromTime) > 0.1) {
          console.log('â° [playOrSimulate] Adjusting currentTime in oncanplay to:', resumeFromTime);
          currentVideo.currentTime = resumeFromTime;
        }
      };
      
      currentVideo.onloadeddata = () => {
        console.log('âœ… [playOrSimulate] Video loaded');
        if (!currentVideo) return;
        
        // resumeFromTimeì´ 0ì´ë©´ ì¦‰ì‹œ ì¬ìƒ (seeked ì´ë²¤íŠ¸ ëŒ€ê¸° ì•ˆ í•¨)
        if (resumeFromTime === 0 && !playbackStarted) {
          startPlayback();
        } else if (resumeFromTime > 0) {
          // resumeFromTimeì´ ìˆìœ¼ë©´ seeked ì´ë²¤íŠ¸ë¥¼ ê¸°ë‹¤ë¦¼
          // currentTimeì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ë‹¤ì‹œ ì„¤ì •
          if (Math.abs(currentVideo.currentTime - resumeFromTime) > 0.1) {
            console.log('â° [playOrSimulate] Setting currentTime in onloadeddata to:', resumeFromTime);
            currentVideo.currentTime = resumeFromTime;
          }
        }
      };
      currentVideo.onerror = (e) => {
        console.error('âŒ Video load failed:', cue.videoUrl, e);
        cleanup();
        // Audioë¡œ fallback (ë¹ˆ ë¬¸ìì—´ ì²´í¬)
        const hasAudioFallback = cue.audioUrl && cue.audioUrl.trim().length > 0;
        if (hasAudioFallback && cue.audioUrl) {
          playAudio(cue.audioUrl, resolve, resumeFromTime);
        } else {
          startSimulation(cue.text.length, resolve);
        }
      };
      
      // ë¹„ë””ì˜¤ ë¡œë“œ ì‹œì‘ (ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ìƒˆë¡œìš´ URL ì¦‰ì‹œ ë¡œë“œ)
      console.log('ğŸ“¥ [playOrSimulate] Video load() called for:', cue.videoUrl);
      currentVideo.load();
    } 
    // Audioë§Œ ìˆìœ¼ë©´ ì˜¤ë””ì˜¤ ì¬ìƒ
    else if (cue.audioUrl) {
      playAudio(cue.audioUrl, resolve, resumeFromTime);
    } 
    // ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜
    else {
      startSimulation(cue.text.length, resolve);
    }
  });
}

// Audio playback helper
function playAudio(audioUrl: string, resolve: () => void, resumeFromTime: number = 0, hasVideo: boolean = false): void {
  // ë¹ˆ ë¬¸ìì—´ ì²´í¬
  if (!audioUrl || audioUrl.trim().length === 0) {
    console.warn('âš ï¸ Empty audioUrl provided to playAudio, skipping');
    resolve();
    return;
  }
  
  // ìºì‹œ ìš°íšŒë¥¼ ìœ„í•´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ (ì„¤ì • ë³€ê²½ ì‹œ ê°•ì œ ìƒˆë¡œê³ ì¹¨)
  const audioUrlWithCacheBust = audioUrl + (audioUrl.includes('?') ? '&' : '?') + '_t=' + Date.now();
  
  currentAudio = new Audio(audioUrlWithCacheBust);
  currentAudio.onloadeddata = () => {
    if (currentAudio) {
      // ì €ì¥ëœ ì‹œê°„ë¶€í„° ì¬ìƒ
      if (resumeFromTime > 0) {
        currentAudio.currentTime = resumeFromTime;
      }
      currentAudio.play();
      console.log('ğŸ”Š Audio playing:', audioUrl, resumeFromTime > 0 ? `from ${resumeFromTime}s` : '', 'Cache bust:', audioUrlWithCacheBust);
      currentAudio.onended = () => {
        cleanup();
        resolve();
      };
    }
  };
  currentAudio.onerror = () => {
    console.warn('âš ï¸ Audio load failed');
    
    // ë¹„ë””ì˜¤ê°€ ìˆìœ¼ë©´ ë¹„ë””ì˜¤ëŠ” ê³„ì† ì¬ìƒí•˜ê³ , ì˜¤ë””ì˜¤ë§Œ ì •ë¦¬
    if (hasVideo && currentVideo) {
      console.log('ğŸ“¹ Video is present, continuing video playback without audio');
      // ì˜¤ë””ì˜¤ë§Œ ì •ë¦¬
      if (currentAudio) {
        currentAudio.pause();
        currentAudio.src = '';
        currentAudio = null;
      }
      // ë¹„ë””ì˜¤ ì¢…ë£Œ ì‹œ resolveí•˜ë„ë¡ ì„¤ì •
      if (!currentVideo.onended) {
        currentVideo.onended = () => {
          cleanup();
          resolve();
        };
      }
    } else {
      // ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ fallback
      console.warn('âš ï¸ No video, falling back to simulation');
      cleanup();
      startSimulation(config?.script[currentIndex]?.text.length || 10, resolve);
    }
  };
  
  // ì˜¤ë””ì˜¤ ë¡œë“œ ì‹œì‘ (ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ìƒˆë¡œìš´ URL ì¦‰ì‹œ ë¡œë“œ)
  currentAudio.load();
  console.log('ğŸ“¥ Audio load() called for:', audioUrl, 'Cache bust:', audioUrlWithCacheBust);
}

// Simulation helper
function startSimulation(textLength: number, resolve: () => void): void {
  const duration = Math.max(2000, textLength * 100);
  simulationDuration = duration;
  simulationStartTime = Date.now();
  simulationElapsedTime = 0;
  
  currentMediaTimeout = setTimeout(() => {
    cleanup();
    resolve();
  }, duration);
}

// Pause current media/simulation
function pauseMediaPlayback(): void {
  // ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ ëª¨ë‘ ì¼ì‹œì •ì§€ (ë™ì‹œ ì¬ìƒ ì¤‘ì¼ ìˆ˜ ìˆìŒ)
  if (currentVideo) {
    pausedMediaTime = currentVideo.currentTime;
    currentVideo.pause();
  }
  if (currentAudio) {
    // ë¹„ë””ì˜¤ê°€ ìˆìœ¼ë©´ ë¹„ë””ì˜¤ ì‹œê°„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ ì‹œê°„ ì‚¬ìš©
    if (!currentVideo) {
      pausedMediaTime = currentAudio.currentTime;
    }
    currentAudio.pause();
  } else if (currentMediaTimeout) {
    // ì‹œë®¬ë ˆì´ì…˜ ì¼ì‹œì •ì§€
    simulationElapsedTime = Date.now() - simulationStartTime;
    clearTimeout(currentMediaTimeout);
    currentMediaTimeout = null;
  }
}

// Resume current media/simulation with updated settings
async function resumeMediaPlayback(): Promise<void> {
  const savedTime = pausedMediaTime;
  const cue = config?.script[currentIndex];
  
  if (!cue) {
    console.warn('âš ï¸ No cue to resume');
    return;
  }
  
  // ê¸°ì¡´ ë¯¸ë””ì–´ ì •ë¦¬
  cleanup();
  
  // videoContainer ì¬í™•ì¸ (React ref ì—…ë°ì´íŠ¸ ëŒ€ë¹„)
  if (config?.videoContainer && !config.videoContainer.parentNode) {
    console.warn('âš ï¸ videoContainer is not in DOM, video may not be visible');
  }
  
  console.log('â–¶ï¸ Resuming with updated settings from:', savedTime);
  
  // ë™ì ìœ¼ë¡œ ë¯¸ë””ì–´ ê²½ë¡œ ì—…ë°ì´íŠ¸ (ì„¤ì • ë³€ê²½ ë°˜ì˜)
  if (config?.getCurrentSettings && config?.workIndex && config?.opponentGender !== undefined) {
    const settings = config.getCurrentSettings();
    
    console.log('ğŸ” [resumeMediaPlayback] Settings received:', {
      hasCustomImage: settings.hasCustomImage,
      personality: settings.selectedPersonality,
      sliderValue: settings.sliderValue
    });
    
    // ìƒëŒ€ì—­ì˜ ëª‡ ë²ˆì§¸ ëŒ€ì‚¬ì¸ì§€ ê³„ì‚° (skipRecording ì œì™¸)
    let opponentDialogueNumber = 0;
    for (let i = 0; i <= currentIndex; i++) {
      const c = config.script[i];
      if (c && c.role !== config.userRole && !c.skipRecording) {
        opponentDialogueNumber++;
      }
    }
    
    // ìƒˆë¡œìš´ ë¯¸ë””ì–´ ê²½ë¡œ ìƒì„± (settingsì—ì„œ hasCustomImage ê°€ì ¸ì˜¤ê¸°)
    const { videoUrl, audioUrl } = getMediaPaths({
      workIndex: config.workIndex,
      opponentGender: config.opponentGender,
      hasCustomImage: settings.hasCustomImage, // âœ… ìµœì‹  ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°
      personality: settings.selectedPersonality || 'basic',
      dialogueNumber: opponentDialogueNumber,
      speed: settings.sliderValue
    });
    
    // cueì— ì—…ë°ì´íŠ¸ëœ ê²½ë¡œ ì ìš© (ìŠ¤í¬ë¦½íŠ¸ ë°°ì—´ì˜ ì›ë³¸ë„ ìˆ˜ì •)
    cue.videoUrl = videoUrl;
    cue.audioUrl = audioUrl;
    
    console.log('âœ… [resumeMediaPlayback] Cue updated:', {
      cueIndex: currentIndex,
      videoUrl: cue.videoUrl,
      audioUrl: cue.audioUrl
    });
    
    console.log('ğŸ›ï¸ Updated media paths for resume:', { 
      videoUrl, 
      audioUrl, 
      hasCustomImage: settings.hasCustomImage,
      personality: settings.selectedPersonality || 'basic',
      dialogueNumber: opponentDialogueNumber, 
      savedTime,
      oldVideoUrl: cue.videoUrl, // ë¹„êµë¥¼ ìœ„í•´
      oldAudioUrl: cue.audioUrl
    });
  } else {
    console.warn('âš ï¸ Cannot update media paths: missing config');
  }
  
  // ìƒˆë¡œìš´ ë¯¸ë””ì–´ë¥¼ ì €ì¥ëœ ì‹œê°„ë¶€í„° ì¬ìƒ (ì—…ë°ì´íŠ¸ëœ ê²½ë¡œë¡œ)
  console.log('â–¶ï¸ Starting playback with updated paths:', {
    videoUrl: cue.videoUrl,
    audioUrl: cue.audioUrl,
    savedTime
  });
  await playOrSimulate(cue, savedTime);
}

// Cleanup media resources
function cleanup(): void {
  if (currentVideo) {
    currentVideo.pause();
    // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ì—ëŸ¬ ë°©ì§€)
    currentVideo.onended = null;
    currentVideo.onerror = null;
    currentVideo.onloadeddata = null;
    currentVideo.onloadedmetadata = null;
    currentVideo.oncanplay = null;
    currentVideo.onseeked = null;
    currentVideo.src = '';
    // DOMì—ì„œ ì œê±°
    if (currentVideo.parentNode) {
      currentVideo.parentNode.removeChild(currentVideo);
    }
    currentVideo = null;
  }
  if (currentAudio) {
    currentAudio.pause();
    currentAudio.src = '';
    currentAudio = null;
  }
  if (currentMediaTimeout) {
    clearTimeout(currentMediaTimeout);
    currentMediaTimeout = null;
  }
  currentMediaResolve = null;
  pausedMediaTime = 0;
  simulationElapsedTime = 0;
}

// Start recording with MediaRecorder + Whisper
async function startRecording(): Promise<void> {
  try {
    console.log('ğŸ¤ [DEBUG] Starting voice recording with MediaRecorder + Whisper...');
    console.log('ğŸ¤ [DEBUG] Current state:', {
      isRecording,
      audioContextState: audioContext?.state,
      audioStreamActive: audioStream?.active,
      hasMediaRecorder: !!mediaRecorder,
      currentPhase,
      vadIntervalRunning: !!vadInterval
    });
    
    // 1. ë…¹ìŒ ì‹œì‘ ì‹œ í”Œë˜ê·¸ ì˜¬ë¦¬ê¸°
    isRecording = true;

    // Ensure AudioContext exists before any VAD work
    try {
      if (!audioContext) {
        const AC = (window as any).AudioContext || (window as any).webkitAudioContext;
        audioContext = new AC();
        console.log('ğŸ”Š [DEBUG] AudioContext created', { sampleRate: audioContext?.sampleRate, state: audioContext?.state });
      } else if (audioContext.state === 'suspended') {
        await audioContext.resume();
        console.log('ğŸ”Š [DEBUG] AudioContext resumed (pre-recording)', { state: audioContext.state });
      } else {
        console.log('ğŸ”Š [DEBUG] AudioContext already active', { state: audioContext.state });
      }
    } catch (e) {
      console.warn('âš ï¸ [DEBUG] Failed to create/resume AudioContext', e);
    }

    // Request microphone access with detailed logging
    console.log('ğŸ” Requesting microphone access...');
    audioStream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: 44100
      } 
    });
    console.log('âœ… Microphone access granted, setting up recording...');

    // MediaRecorder ì„¤ì • ë° ì‹œì‘
    const mime = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') ? 'audio/webm;codecs=opus' : 'audio/webm';
    mediaRecorder = new MediaRecorder(audioStream, { mimeType: mime });
    recordedChunks = [];
    
    mediaRecorder.ondataavailable = (e) => {
      if (e.data && e.data.size > 0) recordedChunks.push(e.data);
    };
    
    mediaRecorder.onstart = () => {
      console.log('âœ… MediaRecorder started', { mimeType: mime });
    };
    
    mediaRecorder.onerror = (e: any) => {
      console.warn('âš ï¸ MediaRecorder error', e);
    };
    
    mediaRecorder.onstop = async () => {
      console.log('â¹ï¸ MediaRecorder stopped, proceeding to next turn immediately');
      const blob = new Blob(recordedChunks, { type: mime });
      
      // ë…¹ìŒ ì •ë¦¬ ë° ë°”ë¡œ ë‹¤ìŒ í„´ìœ¼ë¡œ ì§„í–‰
      cleanupRecording();
      isRecording = false;
      
      console.log('âœ… Recording completed, moving to next turn');
      
      // í˜„ì¬ cueIndex ì €ì¥ (nextCue í˜¸ì¶œ ì „)
      const recordedCueIndex = currentIndex;
      
      // Whisper API ìš”ì²­ì„ Promiseë¡œ ì¶”ì  (ì¦‰ì‹œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ë³€ìˆ˜ì— í• ë‹¹)
      const whisperPromise: Promise<void> = (async () => {
        try {
          const text = await transcribeWithWhisper(blob, 'ko');
          currentUserText = text || '';
          console.log('ğŸ“ Whisper result (background):', currentUserText, 'for cueIndex:', recordedCueIndex);
          
          if (currentUserText) {
            // onSubtitleë¡œ user-final ì „ë‹¬ (ë°±ê·¸ë¼ìš´ë“œ) + cueIndex í¬í•¨
            config?.onSubtitle(currentUserText, 'user-final', recordedCueIndex);
          }
        } catch (err) {
          console.error('âŒ Whisper transcription failed (background)', err);
          config?.onError('stt-failed', err);
        }
      })();
      
      // ì™„ë£Œ í›„ Setì—ì„œ ì œê±°í•˜ëŠ” í•¸ë“¤ëŸ¬ ì¶”ê°€
      whisperPromise.finally(() => {
        pendingWhisperRequests.delete(whisperPromise);
        console.log('âœ… Whisper request completed, remaining:', pendingWhisperRequests.size);
      });
      
      // ì§„í–‰ ì¤‘ì¸ ìš”ì²­ì— ì¶”ê°€
      pendingWhisperRequests.add(whisperPromise);
      console.log('ğŸ“ Added Whisper request, total pending:', pendingWhisperRequests.size);
      
      nextCue(); // ë°”ë¡œ ë‹¤ìŒ í„´ìœ¼ë¡œ
    };
    
    mediaRecorder.start();
    console.log('âœ… Recording started successfully (MediaRecorder)');

    // ìŒì„± ê°ì§€ ì‹œì‘ ë° ì¹¨ë¬µ íƒ€ì´ë¨¸ ì´ˆê¸°í™”
    hasVoiceStarted = false;
    voiceEndTime = null;

    // VADëŠ” ìŒì„±ì„ ê°ì§€í•œ í›„ 3ì´ˆ ì¹¨ë¬µ ì‹œ ìë™ ì¢…ë£Œ
    await setupVAD(true); // ìë™ ì¢…ë£Œ í™œì„±í™”

  } catch (error) {
    console.error('âŒ Recording setup failed:', error);
    if (error instanceof Error) {
      if (error.name === 'NotAllowedError') {
        config?.onError('mic-permission-denied');
      } else if (error.name === 'NotFoundError') {
        config?.onError('mic-permission-denied');
      } else {
        config?.onError('recording-setup-failed', error.message);
      }
    }
    throw error;
  }
}

// Enhanced VAD setup with time domain and better debugging
async function setupVAD(enableAutoStop: boolean = true): Promise<void> {
  try {
    console.log('ğŸ” Setting up Voice Activity Detection (Time Domain)...', { enableAutoStop });

    if (!audioContext) {
      throw new Error('AudioContext is not initialized. Create it in startRecording() before setupVAD().');
    }
    
    // AudioContext ìƒíƒœ í™•ì¸ ë° ì¬ê°œ
    if (audioContext?.state === 'suspended') {
      await audioContext.resume();
      console.log('ğŸ”Š AudioContext resumed');
    }
    
    analyser = audioContext.createAnalyser();
    microphone = audioContext.createMediaStreamSource(audioStream!);

    // VAD ì„¤ì • ìµœì í™”
    analyser.fftSize = 1024; // 2048ì—ì„œ 1024ë¡œ ê°ì†Œ (ë” ë¹ ë¥¸ ì²˜ë¦¬)
    analyser.smoothingTimeConstant = 0.1; // 0.3ì—ì„œ 0.1ë¡œ ê°ì†Œ (ë” ë¯¼ê°)
    analyser.minDecibels = -90;
    analyser.maxDecibels = -10;
    
    // ì˜¤ë””ì˜¤ íŒŒì´í”„ ì—°ê²° í™•ì¸
    microphone.connect(analyser);
    console.log('ğŸ”— Audio pipeline connected:', {
      sourceNode: !!microphone,
      analyserNode: !!analyser,
      audioContextState: audioContext?.state,
      sampleRate: audioContext?.sampleRate
    });

    dataArray = new Uint8Array(analyser.frequencyBinCount);

    vadCalibrationSamples = [];
    isVADCalibrated = false;
    const calibrationStartTime = Date.now();

    console.log('ğŸ¯ VAD calibration started (500ms)...');

    vadInterval = setInterval(() => {
      if (!analyser || !dataArray || isDestroyed) {
        return;
      }

      if (currentPhase !== 'user-recording' || !isRecording) {
        // phaseê°€ ë³€ê²½ë˜ë©´ intervalì€ ê³„ì† ì‹¤í–‰ë˜ì§€ë§Œ early return
        return;
      }

      // ì‹œê°„ ë„ë©”ì¸ ë°ì´í„° ì‚¬ìš© (ë” ì •í™•í•œ ìŒì„± ê°ì§€)
      analyser.getByteTimeDomainData(dataArray as any);

      // RMS ê³„ì‚° (ì‹œê°„ ë„ë©”ì¸)
      let sum = 0;
      let peak = 0;
      for (let i = 0; i < dataArray.length; i++) {
        const normalized = (dataArray[i] - 128) / 128;
        sum += normalized * normalized;
        peak = Math.max(peak, Math.abs(normalized));
      }
      const rms = Math.sqrt(sum / dataArray.length);

      const currentTime = Date.now();
      const isCalibrating = currentTime - calibrationStartTime < VAD_CALIBRATION_TIME;

      if (isCalibrating) {
        vadCalibrationSamples.push(rms);
        console.log('ğŸ¯ VAD calibrating...', { 
          rms: rms.toFixed(4), 
          peak: peak.toFixed(4),
          samples: vadCalibrationSamples.length,
          timeRemaining: VAD_CALIBRATION_TIME - (currentTime - calibrationStartTime),
          rawData: Array.from(dataArray.slice(0, 10)) // ì²˜ìŒ 10ê°œ ìƒ˜í”Œ í™•ì¸
        });
      } else if (!isVADCalibrated) {
        if (vadCalibrationSamples.length > 0) {
          noiseFloor = vadCalibrationSamples.reduce((a, b) => a + b, 0) / vadCalibrationSamples.length;
          // ìµœì†Œ ì„ê³„ê°’ ë³´ì¥
          noiseFloor = Math.max(noiseFloor, VAD_MIN_THRESHOLD);
          isVADCalibrated = true;
          console.log('âœ… VAD calibrated:', { 
            noiseFloor: noiseFloor.toFixed(4), 
            threshold: (noiseFloor * VAD_ADAPTIVE_MULTIPLIER).toFixed(4),
            minThreshold: VAD_MIN_THRESHOLD,
            samples: vadCalibrationSamples.length,
            calibrationData: vadCalibrationSamples.slice(0, 5) // ì²˜ìŒ 5ê°œ ìƒ˜í”Œ í™•ì¸
          });
        } else {
          console.warn('âš ï¸ No calibration samples collected, using default threshold');
          noiseFloor = VAD_MIN_THRESHOLD;
          isVADCalibrated = true;
        }
      } else {
        const threshold = Math.max(noiseFloor * VAD_ADAPTIVE_MULTIPLIER, VAD_MIN_THRESHOLD);
        const isVoiceDetected = rms > threshold;

        // ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸ (ì²˜ìŒ ëª‡ ë²ˆë§Œ)
        // if (Math.random() < 0.1) { // 10% í™•ë¥ ë¡œë§Œ ë¡œê·¸
        //   console.log('ğŸ¯ VAD check:', { 
        //     rms: rms.toFixed(4), 
        //     peak: peak.toFixed(4),
        //     threshold: threshold.toFixed(4), 
        //     isVoiceDetected, 
        //     noiseFloor: noiseFloor.toFixed(4),
        //     silenceDuration: vadSilenceStart ? Date.now() - vadSilenceStart : 0,
        //     rawSample: dataArray[0] // ì²« ë²ˆì§¸ ìƒ˜í”Œ ê°’
        //   });
        // }

        // enableAutoStopì´ falseë©´ ìë™ ì¢…ë£Œí•˜ì§€ ì•ŠìŒ
        if (!enableAutoStop) {
          return; // ì‹¤ì‹œê°„ ë ˆë²¨ í‘œì‹œë§Œì„ ìœ„í•´ VAD ê³„ì† ì‹¤í–‰
        }
        
        if (isVoiceDetected) {
          // ìŒì„± ê°ì§€ë¨
          hasVoiceStarted = true; // ìŒì„±ì´ ì‹œì‘ë˜ì—ˆë‹¤ê³  í‘œì‹œ
          voiceEndTime = null; // ì¹¨ë¬µ ì‹œê°„ ë¦¬ì…‹
          vadSilenceStart = null;
        } else {
          // ì¹¨ë¬µ ê°ì§€ë¨
          if (hasVoiceStarted) {
            // ì´ë¯¸ ìŒì„±ì´ ì‹œì‘ëœ ì ì´ ìˆìœ¼ë©´, ì¹¨ë¬µ ì‹œê°„ ì¹´ìš´íŠ¸ ì‹œì‘
            if (voiceEndTime === null) {
              voiceEndTime = Date.now();
              console.log('ğŸ”‡ Silence started after voice, starting 2 second timer...');
            } else if (Date.now() - voiceEndTime >= 2000) {
              // 2ì´ˆ ì¹¨ë¬µ ì§€ì† â†’ ìë™ ì¢…ë£Œ
              console.log('ğŸ”‡ 2 seconds of silence after voice detected â€” stopping recorder for STT');
              if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
              } else {
                checkRecordingCompletion();
              }
            }
          }
        }
      }
    }, 50); // 100msì—ì„œ 50msë¡œ ê°ì†Œ (ë” ë¹ ë¥¸ ë°˜ì‘)

    console.log('âœ… VAD setup completed');

  } catch (error) {
    console.warn('âš ï¸ VAD setup failed:', error);
  }
}

// Cleanup recording resources
function cleanupRecording(): void {
  console.log('ğŸ§¹ Cleaning up recording...');
  
  if (recordingTimeout) {
    clearTimeout(recordingTimeout);
    recordingTimeout = null;
  }

  if (vadInterval) {
    clearInterval(vadInterval);
    vadInterval = null;
  }

  // MediaRecorder ì •ë¦¬
  if (mediaRecorder && mediaRecorder.state !== 'inactive') {
    try { 
      mediaRecorder.stop(); 
    } catch (e) {
      console.warn('âš ï¸ Error stopping MediaRecorder:', e);
    }
    mediaRecorder = null;
  }
  recordedChunks = [];

  if (audioStream) {
    audioStream.getTracks().forEach(track => track.stop());
    audioStream = null;
  }

  if (audioContext) {
    audioContext.close();
    audioContext = null;
  }

  analyser = null;
  microphone = null;
  dataArray = null;
  isRecording = false;
  vadSilenceStart = null;
  noiseFloor = 0;
  vadCalibrationSamples = [];
  isVADCalibrated = false;
  
  console.log('âœ… Recording cleanup completed');
}

// Cleanup audio resources
function cleanupAudio(): void {
  console.log('ğŸ§¹ Cleaning up audio...');
  if (audioContext) {
    audioContext.close();
    audioContext = null;
  }
  console.log('âœ… Audio cleanup completed');
}

// Check if recording should be completed
function checkRecordingCompletion(): void {
  console.log('ğŸ” CheckRecordingCompletion called:', {
    isRecording,
    currentUserText: currentUserText || '(empty)',
    currentPhase
  });

  if (!isRecording) {
    console.log('â­ï¸ CheckRecordingCompletion skipped: not recording');
    return;
  }

  if (!currentUserText || !currentUserText.trim()) {
    console.log('âš ï¸ No STT text returned (empty). Staying in waiting-for-confirmation to show result.');
    return; // completeRecordingì„ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - ì´ë¯¸ mediaRecorder.onstopì—ì„œ ì²˜ë¦¬ë¨
  }

  console.log('âœ… checkRecordingCompletion: User speech received from Whisper');
  console.log('ğŸ¯ User said:', currentUserText.trim());
  // completeRecordingì„ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - mediaRecorder.onstopì—ì„œ ì²˜ë¦¬
}

// Complete recording
function completeRecording(): void {
  console.log('âœ… Completing recording with text:', currentUserText || '(empty)');
  
  // ì‹¤ì œ ë…¹ìŒëœ í…ìŠ¤íŠ¸ë¥¼ ê³„ì† ë³´ì—¬ì¤Œ (í•˜ë“œì½”ë”©ëœ 'ì‚¬ìš©ì ëŒ€ì‚¬ ì™„ë£Œ' í…ìŠ¤íŠ¸ ì œê±°)
  // í˜„ì¬ ìë§‰ì€ ì´ë¯¸ onSubtitleë¡œ ì„¤ì •ë˜ì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€

  // VAD ì¸í„°ë²Œì„ ë¨¼ì € ì •ë¦¬ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
  if (vadInterval) {
    clearInterval(vadInterval);
    vadInterval = null;
    console.log('ğŸ›‘ VAD interval cleared');
  }

  cleanupRecording();
  
  // isPaused ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì¬ìƒ ì¤‘ì´ë©´ ë‹¤ìŒ cueë¡œ ì§„í–‰
  console.log('ğŸ¯ Complete recording - checking state:', { isPaused, getIsPlaying: config?.getIsPlaying?.() });
  
  if (!isPaused && config?.getIsPlaying()) {
    console.log('âœ… Recording completed, advancing to next cue');
    setTimeout(() => {
      setPhase('waiting'); // waitingìœ¼ë¡œ ì „í™˜í•˜ë©´ setPhaseì—ì„œ nextCue í˜¸ì¶œ
    }, 100);
  } else {
    console.log('â¸ï¸ Recording completed but paused, staying in waiting');
    setPhase('waiting');
  }
}

// Trigger ad-lib mode
async function triggerAdlib(): Promise<void> {
  console.log('ğŸ­ Triggering ad-lib mode...');
  
  try {
    const response = await fetch('/api/next-lines', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        history: config?.script.slice(0, currentIndex + 1),
        userText: currentUserText
      })
    });

    if (response.ok) {
      const data = await response.json();
      console.log('ğŸ­ Ad-lib response:', data);
      // Add new lines to script
      if (data.lines && Array.isArray(data.lines)) {
        config?.script.push(...data.lines);
      }
    }
  } catch (error) {
    console.warn('âš ï¸ Ad-lib request failed:', error);
  }

  completeRecording();
}

// Move to next cue
async function nextCue(): Promise<void> {
  console.log('â­ï¸ nextCue() called, currentIndex:', currentIndex, 'currentPhase:', currentPhase);
  
  if (currentIndex >= (config?.script.length || 0) - 1) {
    console.log('ğŸ Script completed, waiting for pending Whisper requests...');
    
    // ëª¨ë“  Whisper ìš”ì²­ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    if (pendingWhisperRequests.size > 0) {
      console.log('â³ Waiting for', pendingWhisperRequests.size, 'Whisper requests to complete...');
      await Promise.all(Array.from(pendingWhisperRequests));
      console.log('âœ… All Whisper requests completed');
    }
    
    console.log('ğŸ Setting phase to done');
    setPhase('done');
    return;
  }

  currentIndex++;
  console.log('â­ï¸ Processing next cue, new index:', currentIndex);
  processCurrentCue();
}

// Process current cue
function processCurrentCue(): void {
  const cue = config?.script[currentIndex];
  if (!cue) {
    console.log('âŒ No cue found at index:', currentIndex);
    return;
  }

  console.log('ğŸ¬ Processing cue', currentIndex, ':', {
    role: cue.role,
    text: cue.text,
    userRole: config?.userRole
  });
  
  console.log(`ğŸ” Role comparison: cue.role="${cue.role}", userRole="${config?.userRole}", match=${cue.role === config?.userRole}`);

  if (cue.role === config?.userRole) {
    // íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ëŒ€ì‚¬ëŠ” ë…¹ìŒ ìŠ¤í‚µ
    if (cue.skipRecording) {
      console.log('ğŸ‘¤ User turn (special character only) - skipping recording');
      setPhase('user-recording'); // ë‚´ ì°¨ë¡€ ë””ìì¸ ìœ ì§€
      config?.onSubtitle(cue.text, 'ai');
      
      // 2ì´ˆ ëŒ€ê¸° í›„ ë‹¤ìŒ í„´ìœ¼ë¡œ (waiting ë‹¨ê³„ ì—†ì´ ë°”ë¡œ)
      setTimeout(() => {
        console.log('âœ… Special character turn completed, moving to next...');
        if (!isDestroyed && !isPaused) {
          nextCue();
        }
      }, 2000);
    } else {
      console.log('ğŸ‘¤ User turn starting...');
      setPhase('user-recording');
      config?.onSubtitle(cue.text, 'ai');
      // ìë™ìœ¼ë¡œ ë…¹ìŒ ì‹œì‘
      startRecording().catch(error => {
        console.error('âŒ Recording start failed:', error);
        setPhase('waiting');
      });
    }
  } else {
    console.log('ğŸ¤– AI turn starting...');
    
    // ë™ì ìœ¼ë¡œ ë¯¸ë””ì–´ ê²½ë¡œ ì—…ë°ì´íŠ¸ (í•­ìƒ ìµœì‹  ì„¤ì • ë°˜ì˜)
    if (config?.getCurrentSettings && config?.workIndex && config?.opponentGender !== undefined) {
      const settings = config.getCurrentSettings();
      
      console.log('ğŸ” [processCurrentCue] Settings received for new AI turn:', {
        hasCustomImage: settings.hasCustomImage,
        personality: settings.selectedPersonality,
        sliderValue: settings.sliderValue
      });
      
      // ìƒëŒ€ì—­ì˜ ëª‡ ë²ˆì§¸ ëŒ€ì‚¬ì¸ì§€ ê³„ì‚° (skipRecording ì œì™¸)
      let opponentDialogueNumber = 0;
      for (let i = 0; i <= currentIndex; i++) {
        const c = config.script[i];
        if (c && c.role !== config.userRole && !c.skipRecording) {
          opponentDialogueNumber++;
        }
      }
      
      // ê¸°ì¡´ ê²½ë¡œ ì €ì¥ (ë¹„êµìš©)
      const oldVideoUrl = cue.videoUrl;
      const oldAudioUrl = cue.audioUrl;
      
      // ìƒˆë¡œìš´ ë¯¸ë””ì–´ ê²½ë¡œ ìƒì„± (settingsì—ì„œ hasCustomImage ê°€ì ¸ì˜¤ê¸°)
      const { videoUrl, audioUrl } = getMediaPaths({
        workIndex: config.workIndex,
        opponentGender: config.opponentGender,
        hasCustomImage: settings.hasCustomImage, // âœ… ìµœì‹  ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        personality: settings.selectedPersonality || 'basic',
        dialogueNumber: opponentDialogueNumber,
        speed: settings.sliderValue
      });
      
      // cueì— ì—…ë°ì´íŠ¸ëœ ê²½ë¡œ ì ìš© (ìŠ¤í¬ë¦½íŠ¸ ë°°ì—´ì˜ ì›ë³¸ë„ ìˆ˜ì •)
      cue.videoUrl = videoUrl;
      cue.audioUrl = audioUrl;
      
      console.log('ğŸ›ï¸ Updated media paths for new AI turn:', { 
        oldVideoUrl,
        newVideoUrl: videoUrl,
        oldAudioUrl,
        newAudioUrl: audioUrl,
        settings, 
        dialogueNumber: opponentDialogueNumber,
        pathChanged: oldVideoUrl !== videoUrl || oldAudioUrl !== audioUrl
      });
    } else {
      console.warn('âš ï¸ Cannot update media paths in processCurrentCue: missing config');
    }
    
    setPhase('ai-playing');
    config?.onSubtitle(cue.text, 'ai');
    
    playOrSimulate(cue).then(() => {
      console.log('ğŸ¤– AI turn completed, moving to next...');
      // ì¼ì‹œì •ì§€ ìƒíƒœê°€ ì•„ë‹ˆë©´ ë‹¤ìŒìœ¼ë¡œ ì§„í–‰
      if (!isDestroyed && !isPaused) {
        setTimeout(() => {
          if (!isDestroyed && !isPaused) {
            nextCue();
          }
        }, 200);
      } else {
        console.log('â¸ï¸ AI turn ended but paused, not moving to next');
      }
    });
  }
}

// Check play state and handle accordingly
function checkPlayState(): void {
  if (!config) return;
  
  const shouldBePlaying = config.getIsPlaying();
  console.log('ğŸ® Play state check:', { shouldBePlaying, currentPhase });
  
  if (shouldBePlaying && currentPhase === 'idle') {
    console.log('â–¶ï¸ Starting from idle state');
    setPhase('entry');
  } else if (!shouldBePlaying && currentPhase !== 'idle' && currentPhase !== 'done') {
    console.log('â¸ï¸ Pausing from active state');
    setPhase('waiting');
  }
}

// Set phase with logging
function setPhase(phase: Phase): void {
  console.log('ğŸ”„ Phase change:', currentPhase, '->', phase);
  currentPhase = phase;
  config?.onPhase(phase);

  // When entering 'waiting', schedule next cue automatically if playing
  if (phase === 'waiting') {
    setTimeout(() => {
      if (isDestroyed) return;
      console.log('â¸ï¸ Waiting state entered, checking conditions:', { 
        isPaused, 
        getIsPlaying: config?.getIsPlaying?.(), 
        shouldAdvance: !isPaused && config?.getIsPlaying?.() 
      });
      
      if (!config?.getIsPlaying() || isPaused) {
        console.log('â¸ï¸ Still paused; not advancing from waiting.');
        return;
      }
      console.log('â­ï¸ Advancing from waiting to next cue...');
      nextCue();
    }, 200); // small cushion between phases
  }
}

// Create Turn Engine
export function createTurnEngine(engineConfig: TurnEngineConfig): TurnEngine {
  config = engineConfig;
  currentIndex = 0; // ìƒˆë¡œ ì‹œì‘í•  ë•ŒëŠ” 0ë¶€í„°
  currentPhase = 'idle';
  isDestroyed = false;
  isPaused = false; // ì—¬ê¸°ì„œ ì´ˆê¸°í™”

  console.log('ğŸš€ Turn Engine created with config:', {
    scriptLength: config.script.length,
    userRole: config.userRole,
    adlibMode: config.adlibMode,
    currentIndex: currentIndex // í˜„ì¬ ìœ„ì¹˜ ë¡œê¹…
  });

  return {
    load: () => {
      console.log('ğŸ“¥ Turn Engine load called');
      if (currentPhase === 'idle') {
        setPhase('entry');
      }
    },

    start: () => {
      console.log('â–¶ï¸ Turn Engine start called, current phase:', currentPhase);
      
      isPaused = false;
      
      if (currentPhase === 'idle') {
        console.log('ğŸš€ Starting from idle, setting phase to entry');
        setPhase('entry');
        // entryì—ì„œ ë°”ë¡œ ì²« ë²ˆì§¸ cue ì²˜ë¦¬
        setTimeout(() => {
          console.log('ğŸ¬ Processing first cue after entry');
          processCurrentCue();
        }, 100);
      } else if (currentPhase === 'waiting') {
        console.log('ğŸ¬ Resuming from waiting state');
        processCurrentCue();
      } else {
        console.log('âš ï¸ Start called but phase is not idle or waiting:', currentPhase);
      }
    },

    pause: () => {
      console.log('â¸ï¸ Turn Engine pause called, current phase:', currentPhase);
      
      isPaused = true;
      
      if (currentPhase === 'user-recording') {
        cleanupRecording();
        setPhase('waiting');
      } else if (currentPhase === 'ai-playing') {
        // AI ë¯¸ë””ì–´ ì¼ì‹œì •ì§€ (ì •ì§€í•œ ì‹œì  ì €ì¥)
        pauseMediaPlayback();
        console.log('â¸ï¸ AI media paused, staying in ai-playing state');
        // ai-playing ìƒíƒœ ìœ ì§€ (ì¬ê°œ ì‹œ ì´ì–´ì„œ ì§„í–‰)
      } else if (currentPhase === 'waiting') {
        // ì´ë¯¸ ëŒ€ê¸° ìƒíƒœì´ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ)
        console.log('â¸ï¸ Already in waiting state');
      } else if (currentPhase === 'entry') {
        // entry ìƒíƒœì—ì„œ ì¼ì‹œì •ì§€í•˜ë©´ idleë¡œ
        setPhase('idle');
      }
      // idleì´ë‚˜ done ìƒíƒœëŠ” ì´ë¯¸ ì¼ì‹œì •ì§€ ìƒíƒœ
    },

    resume: () => {
      console.log('â–¶ï¸ Turn Engine resume called, current phase:', currentPhase);
      isPaused = false;
      
      if (currentPhase === 'waiting') {
        console.log('â–¶ï¸ Resuming from waiting, processing current cue');
        processCurrentCue();
      } else if (currentPhase === 'ai-playing') {
        // AI í„´ ì¤‘ê°„ì— ì¼ì‹œì •ì§€í–ˆë‹¤ê°€ ì¬ê°œí•˜ëŠ” ê²½ìš°
        console.log('â–¶ï¸ Resuming from ai-playing, continuing playback');
        resumeMediaPlayback().then(() => {
          console.log('ğŸ¤– AI turn resumed and completed, moving to next...');
          // ì¼ì‹œì •ì§€ ìƒíƒœê°€ ì•„ë‹ˆë©´ ë‹¤ìŒìœ¼ë¡œ ì§„í–‰
          if (!isDestroyed && !isPaused) {
            setTimeout(() => {
              if (!isDestroyed && !isPaused) {
                nextCue();
              }
            }, 200);
          } else {
            console.log('â¸ï¸ AI turn ended but paused, not moving to next');
          }
        });
      }
    },

    destroy: () => {
      console.log('ğŸ’¥ Turn Engine destroy called');
      isDestroyed = true;
      cleanupRecording();
      cleanupAudio();
      currentPhase = 'idle';
    },

    getIndex: () => currentIndex,

    manualNext: () => {
      console.log('ğŸ‘† Manual next called');
      if (currentPhase === 'user-recording') {
        completeRecording();
      }
      nextCue();
    },
    
    confirmAndNext: () => {
      console.log('âœ… User confirmed, moving to next cue');
      if (currentPhase === 'waiting-for-confirmation') {
        isStoppedByUser = false;
        nextCue(); // waiting ë‹¨ê³„ ìƒëµí•˜ê³  ë°”ë¡œ ë‹¤ìŒìœ¼ë¡œ
      }
    }
  };
}

// React hook for Turn Engine
export function useTurnEngine(config: TurnEngineConfig) {
  const engineRef = React.useRef<TurnEngine | null>(null);
  const lastPlayStateRef = React.useRef<boolean>(false);

  React.useEffect(() => {
    engineRef.current = createTurnEngine(config);
    return () => {
      if (engineRef.current) {
        engineRef.current.destroy();
      }
    };
  }, []);

  // ì´ë²¤íŠ¸ ê¸°ë°˜ play state ëª¨ë‹ˆí„°ë§ (í´ë§ ì œê±°)
  React.useEffect(() => {
    if (!engineRef.current) return;

    const shouldBePlaying = config.getIsPlaying();
    const lastPlayState = lastPlayStateRef.current;

    // ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ë°˜ì‘
    if (shouldBePlaying !== lastPlayState) {
      console.log('ğŸ® Play state changed:', { from: lastPlayState, to: shouldBePlaying });

      if (shouldBePlaying) {
        engineRef.current.start();
      } else {
        engineRef.current.pause();
      }

      lastPlayStateRef.current = shouldBePlaying;
    }
  }, [config.getIsPlaying]); // config.getIsPlayingì´ ë³€ê²½ë  ë•Œë§Œ ì‹¤í–‰

  return engineRef.current;
}
